---
title: "STAT 4410/8416 Homework 2"
author: "Lefebvre Nelson"
date: "Due on October , 2022"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
library(knitr)
library(magrittr) # needs to be run every time you start R and want to use %>%
library(dplyr)    # alternatively, this also loads %>%
library(ggplot2)
library(reshape2)
library(ggnewscale)
library(maps)
library(usmap)
opts_chunk$set(fig.align='center', message=FALSE, warning=FALSE)
```

1. The data set `airquality` contains the daily air quality measurement in New York from May to Sep 1973.
    ```{r}
    aq <- airquality
    ```
Now answer the following questions:
    a.  Compute the average temperature (`Temp`) for each month. Attach the result to the data frame `aq` as a new column called `monthly_ave_temp`.  Demonstrate your results by showing the head of `aq`.
    ```{r}
    Temperature <- aq$Temp
    aq <- cbind(aq,Temperature)
    colnames(aq)[colnames(aq) == "Temperature"] = "monthly_ave_temp"
    ```
    b.  Draw a plot that displays the relation between the first four variables (`Ozone`, `Solar.R`, `Wind`, and `Temp`). Provide your code as well as your plot. Report highly correlated variables.
    ```{r}
    Ozone <- aq$Ozone
    Solar <- aq$Solar.R
    Wind <- aq$Wind
    Temp <- aq$Temp
    plot(c(Ozone[1:4],Solar[1:4],Wind[1:4],Temp[1:4]),col = c("blue","red","green","orange"))
    ```    
    
    c.  Draw a side-by-side violin plot of solar radiation (`Solar.R`) for each month. Provide your code as well as your plot. Which month is responsible for the highest median solar radiation? How did you deal with the missing values in the `Solar.R` column?
    
    ```{r}
    p <- ggplot(aq, aes(factor(Month), Solar))
p + geom_violin()

    ```    

2. We can generate an $n\times k$ matrix $M$ and a vector $V$ of length $k$ for some specific values of $n$ and $k$ as follows:
    ```{r}
    set.seed(321)
    n = 9
    k = 5
    V = sample(50, size = k, replace = TRUE)
    M = matrix(rnorm(n * k), ncol = k)
    ```
    a.  Now, carefully review the following for-loop. Rewrite the code so that you perform the same job without a loop.  
```{r}
        X = M
        for(i in 1:n) {
          X[i, ] = round(M[i, ] / V, digits = 4)
        }
        X
```


```{r}
    X = round(t(apply(M,1,'/', V)), digits = 4)
    X
```


    b.  Now do the same experiment for $n=900$ and $k=500$. Which runs faster, your code or the for-loop? Demonstrate this using the function `system.time()`.
```{r}
    set.seed(321)
    n = 900
    k = 500
    V = sample(50, size = k, replace = TRUE)
    M = matrix(rnorm(n * k), ncol = k)
    
    X = M
    system.time(for(i in 1:n) {
          X[i, ] = round(M[i, ] / V, digits = 4)
        })
    system.time(round(t(apply(M,1,'/', V)), digits = 4))
#The one who run faster is the 2nd one because it's a optimise/parralelised     
```

3. We want to generate a plot of US arrest data (USArrests). Please provide the detailed codes to answer the following questions.
    a.  Obtain USA state boundary coordinates data for generating a USA map using function `map_data()` and store the data in `mdat`. Display the first few rows of data from `mdat`, noticing that there is a column called `order` that contains the true order of the coordinates.  
    
```{r}
mdat <- map_data("state")
head(mdat)
```    
    
    b.  You will find USA crime data in the data frame called `USArrests`. Standardize the crime rates and create a new column called `state` so that all state names are in lower case. Store this new data in an object called `arrest` and report the first few rows of `arrest`.  
    
```{r}
library(tibble)
USArrests<-USArrests
head(USArrests)
USArrests<- rownames_to_column(USArrests, "state")
USArrests<-USArrests %>%
    as_tibble() %>%
    mutate_at(vars(-state,-UrbanPop), scale, center = TRUE)
arrest<-USArrests%>%mutate(state=tolower(state))
head(arrest)
```
    
    c.  Merge the two data sets `mdat` and `arrest` by state name. Note: merging will change the order of the coordinates data. So, order the data back to the original order and store the merged-ordered data in `odat`. Report the first few rows of data from `odat`.
    
```{r}
odat<-merge(x=mdat,y=arrest,by.x = "region",by.y = "state")
odat<-odat%>%arrange(order)
head(odat)
```     
    
    d.  All the columns of `odat` are not necessary for our analysis. So, obtain a subset by selecting only the columns `long`, `lat`, `group`, `region`, `Murder`, `Assault`, `UrbanPop`, and `Rape`. Store the data in `sdat` and report the first few rows.
    
```{r}
sdat<-odat%>%select(long, lat, group, region, Murder, Assault, UrbanPop, Rape)
head(sdat)
```    
    
    e.  Melt the data frame `sdat` with id variables `long`, `lat`, `group`, `region`. Store the molten data in `msdat` and report the first few rows of data. Use two ways to do this (`reshape2` and `tidyr`).
    
```{r}
msdat <- melt(sdat, id=c("long","lat","group","region"))
head(msdat)
```
    
    f.  The molten data frame `msdat` is now ready to be plotted. Let's use `msdat` from the `reshape2` output. Create a plot showing the USA state map, fill the color by `value`, and `facet_wrap` with `variable`. Please don't add any legend and make sure that facetting labels are identified so that we can compare the facetted plots.
    
```{r}
ggplot(msdat, aes(long, lat)) +
geom_polygon(aes(fill=value, group=group),
data = ~ subset(., variable == "Murder")) +
scale_fill_distiller(palette = "Reds") +
scale_fill_gradient(low="white", high="brown3")+
new_scale_fill() +
geom_polygon(aes(fill=value, group=group),
data = ~ subset(., variable == "Assault")) +
scale_fill_distiller(palette = "Greens") +
scale_fill_gradient(low="white", high="chartreuse3")+
new_scale_fill() +
geom_polygon(aes(fill=value, group=group),
data = ~ subset(., variable == "UrbanPop")) +
scale_fill_distiller(palette = "Blues") +
scale_fill_gradient(low="white", high="blue3")+
new_scale_fill() +
geom_polygon(aes(fill=value, group=group),
data = ~ subset(., variable == "Rape")) +
scale_fill_distiller(palette = "Greys") +
scale_fill_gradient(low="white", high="darkgrey")+
facet_wrap(~ variable)
```    
    
    g. Now examine the plot you have generated in question (f) and answer the following questions based on what you see in the plot.  
        i.  For each crime, name two states with its highest rate.   
        For the rape here is the 2 major state concerned: Nevada and Machigan
        For the Assault here is the 2 major state concerned: Florida and North Carolina
        For the murder here is the 2 major state concerned: Georgia and Missisipi
        
        ii.  Do you think a larger urban population is indicative of a higher murder rate? Why or why not?  
        A larger urban population does not mean a higher murder rate. For example, Mississippi has the highest homicide rate, but their population is relatively less populated than California and New York, as can be seen in the charts. But the filtering of this information seems to me a little too light to make a conclusion. 
        
    h.  In question (b) we standardized the crime rates. Why do you think we did this? Explain what would happen if we did not standardize the data.
    Like this we can conflict the types of crimes comited and their rate. Like this for exemple we can say that the assault rate is pretty high in north carolina but rape rate is pretty low.
    
    
    i.  In question (c) we ordered the data after merging. Why do you think we had to do this? Explain what would happen if we did not.
    We are Merging the map data with the arrests data,because the original row order of the map data was not preserved. So to see data on map, we need to order the outline the maps correctly (joining one co-ordinate with the other).That is why we made use of the order column present in the map data to sort the merged data back to the original order of the map data.If we havent done this it would be impossible to link it to the map.

4. Life expectancy data for four countries can be obtained from the world bank database found at
[github](http://mamajumder.github.io/data-science/data/life-expectancy.csv). It contains life expectancy in years for different genders. Now answer the following questions. 
    a.  Read the data from the above link and display the first few rows of data.
 
    ```{r}
    github <- read.table("http://mamajumder.github.io/data-science/data/life-expectancy.csv", sep = ",")
    plot(github$V1,github$V3,xlab="Date",ylab="Bangladesh")
    ```    
    
    b.  Generate a plot showing trend lines of life expectancy by year. Color them by sex and facet by country. Include your code with the plot.
```{r}
x <- cbind(github$V1,github$V1,github$V1,github$V1, github$V2)
y <- cbind(github$V3,github$V4,github$V5,github$V6, github$V2)
matplot(x,y)
  
```        
    
    c.  Explain what interesting features you noticed in the plot you made in question (b).
    We need to bind all the country to print them.

5. For the following questions please use the data frame `tips`. 
    a.  Create a bar chart that shows the average tip by day.
    
```{r}
    tips <- read.csv("http://www.ggobi.org/book/data/tips.csv")
    library(ggplot2) 
    ggplot(tips, aes(x = day)) +  
  geom_bar(aes(y = sum(..count..)/(..count..)))+ylab("Average tips by day")
```   
    
    b.  Compute the average tip, total tip, and average size grouped by smoker and day. i.e.,  For each combination of smoker and day you should have a row of these summaries. Report these results in a nice table.
    
```{r}
x1 <- ggplot(tips, aes(x = day)) +  
  geom_bar(aes(y = sum(..count..)/(..count..)))+ylab("Average tips by day") + facet_grid(day~smoker)
x2 <-  ggplot(tips, aes(x = day)) +  
  geom_bar(aes(y = (..count..)))+ylab("Total tips by day") + facet_grid(day~smoker)
x3 <-  ggplot(tips, aes(x = day)) +  
  geom_bar(aes(y = (..count..)))+ylab("Average size by day") + facet_grid(size~smoker)

par(mfrow=c(1,3))
plot(x1)
plot(x2)
plot(x3)
``` 
    
    c.  Create a bar chart that shows average tip by day, faceted by smoker.
    
```{r}
x4 <- ggplot(tips, aes(x = day)) +  
  geom_bar(aes(y = sum(..count..)/(..count..)))+ylab("Average tips by day") + facet_grid(~smoker)
plot(x4)
```     
    
    d.  In questions (a) and (c), we plotted a summary of our data which does not show us the whole picture. In practice, we would like to see all of the data. What plot do you suggest would serve a similar purpose to the one in question (c)? In other words, what would be a better plot to show than tips by day, facetted by smoker? Please produce this plot and include your code.
    
I think that one of the possible similar purpose of question c will be the average tips facet by time. Be cause its can show that the restaurant can sell more by dinner or by luanch.

```{r}
x5 <- ggplot(tips, aes(x = day)) +  
  geom_bar(aes(y = sum(..count..)/(..count..)))+ylab("Average tips by day") + facet_grid(~time)
plot(x5)
```  

6.  We have the following data set:
```{r}
    myDat = read.csv("http://mamajumder.github.io/data-science/data/reshape-source.csv")
    myDat <- dcast(melt(myDat, id=c('player', 'track')), player+variable~track)
    kable(myDat)
```
We want to reshape the data and produce the following output:

| player|variable |   A|   B|   C|
|:-----:|:--------|---:|---:|---:|
|      1|walking  | 408| 402| 386|
|      1|cycling  |  43|  31|  41|
|      2|walking  | 373| 404| 422|
|      2|cycling  |  53|  41|  30|
|      3|walking  | 403| 393| 422|
|      3|cycling  |  25|  46|  48|

Provide code that will produce this desired output. Demonstrate your answer by displaying the output as well.

7. **Ordering the factor** In class, we have seen how to order factors. Suppose we have the following data about a certain value obtained during particular months of the year;
```{r}
    month = c("July", "June", "September", "May", "October", "August")
    value = c(35, 72, 14, 23, 60, 105)
    df = data.frame(month, value)
```
Now please do the following:  
    a.  Convert the month column of the data frame `df` into a factor column. Demonstrate that it is indeed converted into a factor column.
    
```{r}
    df$month <- as.factor(df$month)
```    
    
    b.  Now generate a bar chart showing the value for different months.
    
```{r}
    barplot(value,names.arg=month,xlab="Month",ylab="Value",col=heat.colors(length(value), alpha=1))
```    
    
    c.  Notice the order of the levels of the months is not natural, instead the plot shows the dictionary order. Now, order the bars according to the natural order of the levels of the class (months of the year as they appear in chronological order) and regenerate the bar graph.  
    
```{r}
    library(magrittr) # needs to be run every time you start R and want to use %>%
    library(dplyr)    # alternatively, this also loads %>%
    df <- df %>% arrange(match(df$month, month.name))
    barplot(df$value,names.arg=df$month,xlab="Month",ylab="Value",col=heat.colors(length(value), alpha=1))
```

8.  Install the `babynames` package with `install.packages()`. This package includes data from the Social Security Administration about American baby names over a wide range of years. Generate a plot of the reported proportion of babies born with the name `Angelica` over time. Do you notice anything odd about the plotted data? (Hint: you should.) If so, describe the issue and generate a new plot that adjusts for this problem. Make sure you show both plots along with all code that was used to generate them.

```{r}
    #install.packages("babynames")
```

```{r}
    NameBaby <- babynames::babynames
    names <- NameBaby$name
    Year <- NameBaby$year
    Rank <- NameBaby$n
    # At the begin i was using this : 
    #plot(Year,data = names["Angelica"])
    #The problem was the axis was reversed and it's taking a lot of time to generate, and also seems not accurate..
    
    # SO instead i use the library magrittr and dplyr, but also i choose only the Female person who have this name because it's more a girl name. (And the graph was not readable because the guys and girls who name Angelica was mixed)
    NameBaby %>%
      filter(name == "Angelica", sex == "F") %>%
      ggplot() + geom_point(mapping = aes(x = year, y = n), col="Red") + ylab("Number of Angelica")
```

9. Suppose we have a vector of data as follows:
```{r}
    myVector = c(-15, -10, -5, 0, 5, 10, 15, 20)
```
    a.  Using the function `tapply()`, separately compute the means of the first three values, next two values, and the last three values of `myVector`. Show your code. Your result should be: -10.0, 2.5, 15.0.
```{r}
    myVector = c(-15, -10, -5, 0, 5, 10, 15, 20)
    vector2 = c(1,1,1,2,2,3,3,3)
    tapply(myVector,vector2, mean)
```    
    
    b.  Now repeat question (a), but instead of computing means, you will compute the sum of squares. Again, show your code. Your result should be: 350, 25, 725.
```{r}
    myVector = c(-15, -10, -5, 0, 5, 10, 15, 20)
    vector2 = c(1,1,1,2,2,3,3,3)
    tapply(myVector^2,vector2, sum)
```   
